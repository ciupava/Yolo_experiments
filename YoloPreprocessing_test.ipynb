{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "bNcLvhZy8C0a"
      ],
      "mount_file_id": "1sBmr_L2NU-tc5bxW9_Vl_NwVIDSqJ5uE",
      "authorship_tag": "ABX9TyOgkj9XkCdxZv6Be1omgz1O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ciupava/Yolo_experiments/blob/main/YoloPreprocessing_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Preprocessing** ###\n",
        "\n",
        "This notebook adapts the one provided by Omdena to HOT as an outcome of the challenge who took place in the summer 2024.\n",
        "(The original notebook downloads and preprocesses the RAMP data package (RAMP-DATA-V0), which comprises over 100k image-label in TIF-GEOJSON formats and 22 regional folders. The resulting output of that notebook was YOLO-DATA-V1 dataset. YOLO-DATA-V2 and YOLO-DATA-V3 datasets were created using the Pruning notebook)\n",
        "\n",
        "Summary of the tasks accomplished in the notebook:\n",
        "\n",
        "1. Data Exploration\n",
        "    \n",
        " - Obtain data from the dedicated folder (currently in HOT's Google Drive).\n",
        " - Complete basic data integrity checks.\n",
        " - Count the image and label files and confirm that each image has a label with the matching name.\n",
        " - Check the shape of the images.\n",
        "\n",
        "~2. Enhance the dataset~\n",
        " - ~Create a dataframe for analysis and data control.~\n",
        " - ~Set Shanghai and Paris subfolders from the dataset by flagging 'use' to False for image shape is not (256, 256, 3).~\n",
        " - ~Set 'use' flag to False randomly for 13% of background images to reduce from 18% to 5%.~\n",
        " - ~Identify images found with incorrect image-label in the exclusions_list.txt~\n",
        "\n",
        "2. Data wrangling\n",
        "\n",
        " - Assess all the folders, one per city\n",
        " - Get the list of the split train/valid for the set of images as it was used on the first experiment run on RAMP metric\n",
        " -\n",
        "\n",
        "3. Convert data (TIF/GEOJSON) to YOLO (JPG/TXT)\n",
        "\n",
        " - ~Image-label pairs were randomly assigned to train-val-test folders using a 70-15-15 split.~\n",
        " - Generate text files based on the YOLO's segmentation format from the GEOJSON and TIF files. Polygons were extracted from each GEOJSON file and aligned with the location information embedded in the TIF. Note: 0,0 is the top, left, and coordinates normalized over 0, 1.\n",
        " - Convert the TIF images to JPG at quality level 100.\n",
        " - Save to separate folders (one per city)\n",
        "\n",
        "\n",
        "Note: A GPU is not required for preprocessing. Due to the numerous disk i/o operations, this notebook is preferred over Google Colab for local running."
      ],
      "metadata": {
        "id": "mnZ8mkSnYx4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initial import"
      ],
      "metadata": {
        "id": "ZAv0e7Wnk8VN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -q rasterio\n",
        "!pip3 install -q pyproj"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uqDxaw9Y5-m",
        "outputId": "2406caeb-63ac-4bd3-8b03-2c861a50563f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import json\n",
        "import cv2\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from pyproj import Transformer\n",
        "import rasterio\n",
        "import matplotlib.image as mpimg\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import yaml\n",
        "import gc\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "nVmpJK84Y9fY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connecting to 'My Drive', to be able to access local data\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9FgZHOxY_ul",
        "outputId": "63712219-5668-4bea-b9b8-77b4f43d0d30"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/drive/My Drive/YOLO_test/data/metric_test_data/')\n",
        "os.listdir()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkQOogTiZ4vJ",
        "outputId": "a959ccdc-ab86-459c-f673-c0bbf325505c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['model51_td364',\n",
              " 'model95_td370',\n",
              " 'model97_td372',\n",
              " 'model98_td373',\n",
              " 'model102_td391',\n",
              " 'model108_td529',\n",
              " 'model110_td394',\n",
              " 'model112_td397',\n",
              " 'model113_td398',\n",
              " 'model114_td399',\n",
              " 'model134_td456',\n",
              " 'model135_td459',\n",
              " 'model136_td462',\n",
              " 'model137_td463',\n",
              " 'model147_td485',\n",
              " 'model148_td488',\n",
              " 'model149_td489',\n",
              " 'model158_td508',\n",
              " 'model162_td530',\n",
              " 'model163_td523',\n",
              " 'model164_td524',\n",
              " 'model165_td525',\n",
              " 'model166_td526',\n",
              " 'model167_td528',\n",
              " 'model168_td539']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining path variables"
      ],
      "metadata": {
        "id": "HaFwnTdZkyCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = f\"{os.getcwd()}\"\n",
        "print(f\"\\n---\\nCurrent working directory {base_path}\")\n",
        "\n",
        "list_filename_tent = os.path.join(os.getcwd(), \"../../cities_list.txt\")\n",
        "print(f'Tentative: {os.path.normpath(list_filename_tent)}')\n",
        "list_filename = os.path.normpath(os.path.join(os.getcwd(), \"../../cities_list.txt\"))\n",
        "print(f'File with list of city names to be used: {list_filename}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlqHLgyMkxVF",
        "outputId": "2a187aa9-2d7b-4f44-de09-0021e2365331"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---\n",
            "Current working directory /content/drive/MyDrive/YOLO_test/data/metric_test_data\n",
            "Tentative: /content/drive/MyDrive/YOLO_test/cities_list.txt\n",
            "File with list of city names to be used: /content/drive/MyDrive/YOLO_test/cities_list.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check that training-datasets list file exists and is readable\n",
        "if not os.path.exists(f'{list_filename}'):\n",
        "    raise ValueError(f\"Can't find file {list_filename}\")\n",
        "\n",
        "### generating list of regions (cities) from the input txt file\n",
        "# input text file from command line:\n",
        "# list_filename = \"cities_list.txt\"\n",
        "\n",
        "#TO_DO: add condition of stopping if filename empty or file doesn't exist\n",
        "print(f\"---\\nI am going to get the names from {list_filename} (name of the list file you provided)\")\n",
        "# with open(\"cities_list.txt\", \"r\") as file:\n",
        "#     cities_list = \"\".join(file.read().split(\"\\n\"))\n",
        "# the following is to obtain the list of cities, removing commented lines (starting with \"#\"):\n",
        "with open(list_filename, 'r') as f:\n",
        "    full_file = f.read()\n",
        "    # print(full_file)\n",
        "    full_list = full_file.split('\\n') # separating per each new line\n",
        "    cities_list = []\n",
        "    for counter in range(len(full_list)):\n",
        "        line = full_list[counter]\n",
        "        # print(f\"line is {line}\")\n",
        "        if not line.startswith('#'): # this is to avoid commented lines in the input file\n",
        "            cities_list.append(full_list[counter])\n",
        "print(f\"---\\nList of cities: {cities_list}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOFOW2fUjg-s",
        "outputId": "dc73e252-d398-4ee8-90e3-0991e6061245"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "I am going to get the names from /content/drive/MyDrive/YOLO_test/cities_list.txt (name of the list file you provided)\n",
            "---\n",
            "List of cities: ['model51_td364']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining functions\n"
      ],
      "metadata": {
        "id": "KOAyorSYq7Wx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to create a list of the labels and rgs files with path"
      ],
      "metadata": {
        "id": "MPTuxamArDu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Changed from Omdena's original version, to keep into account different folder structure\n",
        "def find_files():\n",
        "    \"\"\"\n",
        "    Find chip (.tif) and label (.geojson) files in the specified folders.\n",
        "\n",
        "    Returns:\n",
        "    cwps (list): List of chip filenames with path.\n",
        "    lwps (list): List of label filenames with path.\n",
        "    base_folders (list): List of base folder names.\n",
        "    \"\"\"\n",
        "\n",
        "    # Find the folders\n",
        "    data_folders = glob.glob(base_path)\n",
        "\n",
        "    # Create a list to store chip (.tif), mask (.mask.tif), and label (.geojson) filenames with path\n",
        "    cwps = []\n",
        "    lwps = []\n",
        "\n",
        "    # Create a list to store the base folder names\n",
        "    base_folders = []\n",
        "\n",
        "    for folder in data_folders:\n",
        "        print(f'folder is {folder}')\n",
        "        # Pattern to match all .tif files in the current folder, including subdirectories\n",
        "        tif_pattern = f\"{folder}/**/**/**/*.tif\"\n",
        "        print(f'tif pattern is {tif_pattern}')\n",
        "        print(len(tif_pattern))\n",
        "        # Find all .tif files in the current 'training*' folder and its subdirectories\n",
        "        found_tif_files = glob.glob(tif_pattern, recursive=True)\n",
        "        print(f'found tif files {found_tif_files}')\n",
        "        print(len(found_tif_files))\n",
        "        # Filter out .mask.tif files and add the rest to the tif_files list\n",
        "        for file in found_tif_files:\n",
        "            if not file.endswith('mask.tif'):\n",
        "                cwps.append(file)\n",
        "\n",
        "        # Pattern to match all .geojson files in the current folder, including subdirectories\n",
        "        geojson_pattern = f\"{folder}/**/**/**/*.geojson\"\n",
        "        print(f'geojson pattern is {geojson_pattern}')\n",
        "        print(len(geojson_pattern))\n",
        "        # Find all .geojson files\n",
        "        found_geojson_files = glob.glob(geojson_pattern, recursive=True)\n",
        "        print(f'found gjson files {found_geojson_files}')\n",
        "        print(len(found_geojson_files))\n",
        "        # Add found .geojson files to the geojson_files list\n",
        "        lwps.extend(found_geojson_files)\n",
        "\n",
        "    # Sort the lists\n",
        "    cwps.sort()\n",
        "    lwps.sort()\n",
        "\n",
        "    # Assert that the the number files for each type are the same\n",
        "    assert len(cwps) == len(lwps), \"Number of tif files and label files do not match\"\n",
        "\n",
        "    # Function to check that the filenames match\n",
        "    for n, cwp in enumerate(cwps):\n",
        "        c = os.path.basename(cwp).replace('.tif', '')\n",
        "        l = os.path.basename(lwps[n]).replace('.geojson', '')\n",
        "\n",
        "        assert c == l, f\"Chip and label filenames do not match: {c} != {l}\"\n",
        "\n",
        "        base_folders.append(cwp.split('/')[1])\n",
        "\n",
        "    return cwps, lwps, base_folders\n",
        "\n",
        "# # Call the function and print the number of found files\n",
        "# cwps, lwps, base_folders = find_files()  ##### MOVED TO AFTER !!!! ######\n",
        "# print('Found {} chip files'.format(len(cwps)))\n",
        "# N = 6\n",
        "# test_list_cwps = cwps[:N]\n",
        "# print(f\"The first {N} elements of list are : {str(test_list_cwps)}\")\n",
        "# print('Found {} label files\\n'.format(len(lwps)))\n",
        "# test_list_lwps = lwps[:N]\n",
        "# print(f\"The first {N} elements of list are : {str(test_list_lwps)}\")\n",
        "# # Print message if all filenames match\n",
        "# print('All filenames match; each tif has a label!')"
      ],
      "metadata": {
        "id": "0OmIA6SNSrz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to check the data shapes and store them in a dictionary"
      ],
      "metadata": {
        "id": "BiNNoe9-0wa_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_shapes(iwps):\n",
        "    \"\"\"\n",
        "    Check the shapes of image files and store them in a dictionary.\n",
        "\n",
        "    Parameters:\n",
        "    iwps (list): A list of image files with paths.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing two elements:\n",
        "        - shapes_dict (dict): A dictionary where the keys are the image shapes and the values are the counts.\n",
        "        - shapes (list): A list of the shapes of the chip files in the same order as the input list.\n",
        "    \"\"\"\n",
        "    # Create a dictionary to store the shape of the chip files\n",
        "    shapes_dict = {}\n",
        "    shapes = []\n",
        "\n",
        "    for iwp in tqdm(iwps):\n",
        "        # Read the chip file\n",
        "        shape = cv2.imread(iwp, -1).shape\n",
        "\n",
        "        # Store the shape in the dictionary\n",
        "        if str(shape) in shapes_dict:\n",
        "            shapes_dict[str(shape)] += 1\n",
        "        else:\n",
        "            shapes_dict[str(shape)] = 1\n",
        "\n",
        "        shapes.append(shape)\n",
        "\n",
        "    # Return the dictionary\n",
        "    return shapes_dict, shapes\n",
        "\n",
        "# # Get shapes data     ##### MOVED TO AFTER !!!! ######\n",
        "# shapes_data = check_shapes(cwps)\n",
        "\n",
        "# # Print the shape of the first chip file\n",
        "# print(f'Chip shapes with counts are: {shapes_data[0]}')"
      ],
      "metadata": {
        "id": "Vsn5Sf8F04Fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A few functions to work on the geospatial data to make it available for the Yolo format\n",
        "\n",
        "i.e. the tif become a jpg and the geojson become a txt"
      ],
      "metadata": {
        "id": "lE0Vtx911OqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_geo_data(iwp):\n",
        "    \"\"\"\n",
        "    Extracts geo data from a geotif.\n",
        "\n",
        "    Parameters:\n",
        "    iwp (str): The image file with path.\n",
        "\n",
        "    Returns:\n",
        "    dict: A dictionary containing the extracted geo data. The dictionary includes the following keys:\n",
        "        - 'left': The left coordinate of the bounding box.\n",
        "        - 'right': The right coordinate of the bounding box.\n",
        "        - 'top': The top coordinate of the bounding box.\n",
        "        - 'bottom': The bottom coordinate of the bounding box.\n",
        "        - 'width': The width of the bounding box.\n",
        "        - 'height': The height of the bounding box.\n",
        "        - 'crs': The coordinate reference system (CRS) of the geotif.\n",
        "    \"\"\"\n",
        "    # Open the image file in binary mode ('rb') for reading Exif data\n",
        "    with rasterio.open(iwp) as src:\n",
        "\n",
        "        if src.crs is None:\n",
        "            raise ValueError(\"No CRS found in the image file. Please check the file and try again.\")\n",
        "        elif src.bounds is None:\n",
        "            raise ValueError(\"No bounds found in the image file. Please check the file and try again.\")\n",
        "\n",
        "        # Convert the bounds to the expected format\n",
        "        transformer = Transformer.from_crs(src.crs, 'EPSG:4326')\n",
        "        left, bottom = transformer.transform(src.bounds.left, src.bounds.bottom)\n",
        "        right, top = transformer.transform(src.bounds.right, src.bounds.top)\n",
        "        width = right - left\n",
        "        height = top - bottom\n",
        "\n",
        "        # Collect and return the extracted geo data\n",
        "        results = {'left': left, 'right': right, 'top': top, 'bottom': bottom, 'width': width, 'height': height, 'crs': src.crs.to_string()}\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "6g9Tr4Ja2IUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_and_clamp(values):\n",
        "    \"\"\"\n",
        "    Check and clamp the values in a nested list.\n",
        "\n",
        "    Parameters:\n",
        "    values (list): A nested list of values to be checked and clamped.\n",
        "\n",
        "    Returns:\n",
        "    list: A nested list of clamped values.\n",
        "\n",
        "    \"\"\"\n",
        "    # Initialize an empty list to store the clamped values\n",
        "    clamped_values = []\n",
        "\n",
        "    # Iterate over each sublist in the list\n",
        "    for sublist in values:\n",
        "        # Use a list comprehension to check and clamp each value in the sublist\n",
        "        clamped_sublist = [[max(0, min(1, value)) for value in pair] for pair in sublist]\n",
        "\n",
        "        # Add the processed sublist to the clamped_values list\n",
        "        clamped_values.append(clamped_sublist)\n",
        "\n",
        "    return clamped_values\n",
        "\n",
        "# Example usage\n",
        "# This function clamps polygons coordinates that go outside of the coordinate bounds of the chip.\n",
        "# YOLO requires all coordinates be in the range of (0, 1)\n",
        "# test = [[[0.998301, 0.642099], [0.997246, 1.417954], [-0.99515, 0.404696], [0.997182, 0.40438], [0.996855, 0.334872], [0.926036, 0.346056], [0.973166, 0.646062], [0.998301, 0.642099]]]\n",
        "# print(check_and_clamp(test))"
      ],
      "metadata": {
        "id": "OjrXRgdW1YWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = [[[0.998301, 0.642099], [0.997246, 1.417954], [-0.99515, 0.404696], [0.997182, 0.40438], [0.996855, 0.334872], [0.926036, 0.346056], [0.973166, 0.646062], [0.998301, 0.642099]]]\n",
        "print(check_and_clamp(test))"
      ],
      "metadata": {
        "id": "dT785NHg2UKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten_list(nested_list):\n",
        "    \"\"\"\n",
        "    Flattens a nested list into a single flat list.\n",
        "\n",
        "    Parameters:\n",
        "    nested_list (list): The nested list to be flattened.\n",
        "\n",
        "    Returns:\n",
        "    list: The flattened list.\n",
        "    \"\"\"\n",
        "    flat_list = []\n",
        "\n",
        "    # Iterate over all the elements in the given list\n",
        "    for item in nested_list:\n",
        "        # Check if the item is a list itself\n",
        "        if isinstance(item, list):\n",
        "            # If the item is a list, extend the flat list by adding elements of this item\n",
        "            flat_list.extend(flatten_list(item))\n",
        "        else:\n",
        "            # If the item is not a list, append the item itself\n",
        "            flat_list.append(item)\n",
        "    return flat_list"
      ],
      "metadata": {
        "id": "LJTh0zeq2Uk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_coordinates(coordinates, geo_dict):\n",
        "    \"\"\"\n",
        "    Convert coordinates from one coordinate system to another based on the provided geo_dict.\n",
        "\n",
        "    Args:\n",
        "        coordinates (list): A list of coordinate sets.\n",
        "        geo_dict (dict): A dictionary containing information about the coordinate system.\n",
        "\n",
        "    Returns:\n",
        "        list: The converted coordinates.\n",
        "\n",
        "    Raises:\n",
        "        AssertionError: If the maximum coordinate value is greater than 1 or the minimum coordinate value is less than 0.\n",
        "    \"\"\"\n",
        "    # Iterate over the outer list\n",
        "    for i in range(len(coordinates)):\n",
        "        # Iterate over each coordinate set in the inner list\n",
        "        for j in range(len(coordinates[i])):\n",
        "            if geo_dict['crs'] == 'EPSG:4326':\n",
        "                # Convert the coordinates for the EPSG:4326\n",
        "                coordinates[i][j] = [round((coordinates[i][j][0] - geo_dict['left'])/geo_dict['width'], 6), \\\n",
        "                                    round((geo_dict['top'] - coordinates[i][j][1])/geo_dict['height'], 6)]\n",
        "            else:\n",
        "                # Convert the coordinates for not EPSG:4326\n",
        "                coordinates[i][j] = [round((coordinates[i][j][0] - geo_dict['bottom'])/geo_dict['height'], 6), \\\n",
        "                                    round((geo_dict['right'] - coordinates[i][j][1])/geo_dict['width'], 6)]\n",
        "\n",
        "    coordinates = check_and_clamp(coordinates)\n",
        "\n",
        "    # Make sure that the coordinates are within the expected range\n",
        "    assert max(flatten_list(coordinates)) <= 1, \"The maximum coordinate value is greater than 1\"\n",
        "    assert min(flatten_list(coordinates)) >= 0, \"The minimum coordinate value is less than 0\"\n",
        "\n",
        "    return coordinates"
      ],
      "metadata": {
        "id": "VkWmdu-62ZYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to write LABELS to Yolo format"
      ],
      "metadata": {
        "id": "DSA7efmu2hHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def write_yolo_file(iwp, folder, class_index=0):\n",
        "    \"\"\"\n",
        "    Writes YOLO label file based on the given image with path and class index.\n",
        "\n",
        "    Args:\n",
        "        iwp (str): The image with path.\n",
        "        class_index (int, optional): The class index for the YOLO label. Defaults to 0.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the GeoJSON filename with path from the chip filename with path\n",
        "    # lwp = iwp.replace(\".tif\", \".geojson\").replace(\"source\", \"labels\")\n",
        "    lwp = iwp.replace(\".tif\", \".geojson\").replace(\"chips\", \"labels\") # CHANGED HERE !!!!!! ##########\n",
        "\n",
        "    # Create the YOLO label filename with path from the chip filename with path\n",
        "    ywp = os.path.join(f'ramp_data_yolo/folder/labels/', iwp.split('/')[-1].replace('.tif', '.txt')).replace('folder', folder)\n",
        "\n",
        "    # Create the YOLO label folder if it does not exist\n",
        "    os.makedirs(os.path.dirname(ywp), exist_ok=True)\n",
        "\n",
        "    # Remove the YOLO label file if it already exists\n",
        "    if os.path.exists(ywp):\n",
        "        os.remove(ywp)\n",
        "\n",
        "    # Fetch the chip's Exif data\n",
        "    geo_dict = get_geo_data(iwp)\n",
        "\n",
        "    # Open the GeoJSON file\n",
        "    with open(lwp, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    # Initialize the polygon count\n",
        "    polygon_count = 0\n",
        "\n",
        "    # Navigate through the GeoJSON structure\n",
        "    for feature in data['features']:\n",
        "        if feature['geometry']['type'] == 'Polygon':\n",
        "            # Increment the polygon count\n",
        "            polygon_count += 1\n",
        "\n",
        "            # Get the coordinates of the polygon\n",
        "            coordinates = feature['geometry']['coordinates']\n",
        "\n",
        "            # Convert the coordinates\n",
        "            new_coordinates = flatten_list(convert_coordinates(coordinates, geo_dict))\n",
        "            new_coordinate_str = ' '.join(map(str, flatten_list(new_coordinates)))\n",
        "\n",
        "            # Write the converted coordinates to a file\n",
        "            with open(ywp, 'a+') as file:\n",
        "                # Move the file pointer to the start of the file to check its contents.\n",
        "                file.seek(0)  # Go to the beginning of the file\n",
        "                first_character = file.read(1)  # Read the first character to determine if the file is empty\n",
        "\n",
        "                # If the first character does not exist, the file is empty\n",
        "                if not first_character:\n",
        "                    # Write the first string without a new line before it\n",
        "                    file.write(f'{class_index} ' + new_coordinate_str)\n",
        "\n",
        "                else:\n",
        "                    # The file is not empty, write the new string on a new line\n",
        "                    file.write(f'\\n{class_index} ' + new_coordinate_str)\n",
        "\n",
        "    if polygon_count == 0:\n",
        "        # Open the file in write mode, which creates a new file if it doesn't exist\n",
        "        with open(ywp, 'w') as file:\n",
        "            pass  # No need to write anything, just creating the file"
      ],
      "metadata": {
        "id": "OfkLUFej2gmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to save TIFs to Yolo format"
      ],
      "metadata": {
        "id": "PRj4lrpj2xUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_tif_to_jpg(cwp, folder, ql=100):\n",
        "    \"\"\"\n",
        "    Converts a TIFF image file to JPEG format.\n",
        "\n",
        "    Parameters:\n",
        "    cwp (str): The path to the TIFF image file.\n",
        "    ql (int): The quality level of the JPEG image (default is 100).\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    # Open the tif image file\n",
        "    with Image.open(cwp) as img:\n",
        "        # Convert the image to RGB and save it as a JPEG\n",
        "        rgb_img = img.convert('RGB')\n",
        "\n",
        "        # Define the output path with .jpg extension\n",
        "        jwp = os.path.join('ramp_data_yolo/folder/images/', cwp.split('/')[-1].replace('.tif', '.jpg')).replace('folder', folder)\n",
        "\n",
        "        # Create the output folder if it does not exist\n",
        "        os.makedirs(os.path.dirname(jwp), exist_ok=True)\n",
        "\n",
        "        # Save the image at quality level ql\n",
        "        rgb_img.save(jwp, \"JPEG\", quality=ql)\n",
        "\n",
        "        # Print the output path\n",
        "        return (f'Writing: {jwp}')"
      ],
      "metadata": {
        "id": "a-4CLoBl2xAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Not used functions"
      ],
      "metadata": {
        "id": "bNcLvhZy8C0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WE MIGHT NOT NEED THIS ONE, AS IT'S ONLY USED ONCE IN THE FUNCTION (NOT USED) BELOW\n",
        "\n",
        "def get_polygon_count(lwp):\n",
        "    \"\"\"\n",
        "    Count the number of polygons in a GeoJSON file.\n",
        "\n",
        "    Parameters:\n",
        "    lwp (str): The path to the GeoJSON file.\n",
        "\n",
        "    Returns:\n",
        "    int: The number of polygons in the GeoJSON file.\n",
        "    \"\"\"\n",
        "    # Open the GeoJSON file\n",
        "    with open(lwp, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    # Initialize the polygon count\n",
        "    polygon_count = 0\n",
        "\n",
        "    # Navigate through the GeoJSON structure\n",
        "    for feature in data['features']:\n",
        "        if feature['geometry']['type'] == 'Polygon':\n",
        "            # Increment the polygon count\n",
        "            polygon_count += 1\n",
        "\n",
        "    return polygon_count"
      ],
      "metadata": {
        "id": "YjkB_-3Uc0r5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  WE DO NOT NEED THIS ONE\n",
        "\n",
        "# 1. Create a DataFrame with the filenames, basefolders, polygon counts, and image shapes.\n",
        "# 2. Add a 'use' field that will be set to False for image-label pairs that will not be used.\n",
        "#     a. Randomly set 'use' to False to reach the background images to the Z_TARGET of 5%.\n",
        "#     b. Set 'use' to False when shape is not 256, 256, 3.\n",
        "# 3. Review counts to verify the changes.\n",
        "\n",
        "Z_TARGET_PERCENTAGE = 0.05\n",
        "\n",
        "# Create a list to store the polygon counts\n",
        "polygon_counts = []\n",
        "\n",
        "# Loop through the label files and store the polygon count of each file\n",
        "for lwp in tqdm(lwps):\n",
        "    polygon_counts.append(get_polygon_count(lwp))\n",
        "\n",
        "# Create a list of chip and label filenames\n",
        "fwps = [os.path.basename(lwp).replace('.geojson', '.tif') for lwp in lwps]\n",
        "\n",
        "# Create a DataFrame to store the chip, mask, and label filenames and the polygon counts\n",
        "df = pd.DataFrame({\n",
        "    'base_folder': base_folders,\n",
        "    'cwp': cwps,\n",
        "    'fwp': fwps,\n",
        "    'polygon_count': polygon_counts,\n",
        "    'shape': shapes_data[1],\n",
        "    'use': True\n",
        "})\n",
        "\n",
        "# Figure out how many zero count images to use\n",
        "z_count = len(df[df['polygon_count'] == 0])\n",
        "z_target = int(round(len(df)*Z_TARGET_PERCENTAGE, 0))\n",
        "\n",
        "# Filter the DataFrame where polygon_count is 0\n",
        "condition = df['polygon_count'] == 0\n",
        "subset_df = df[condition]\n",
        "\n",
        "# # Randomly select 5001 indices from this subset and set use to True\n",
        "# indices_to_set_true = np.random.choice(subset_df.index, size=5001, replace=False)\n",
        "# ValueError: Cannot take a larger sample than population when 'replace=False'\n",
        "# Randomly select 200 indices from this subset and set use to True (setting to 200, as the images are 4200)\n",
        "indices_to_set_true = np.random.choice(subset_df.index, size=200, replace=False)\n",
        "df.loc[indices_to_set_true, 'use'] = True\n",
        "\n",
        "# Set 'use' to False for the remaining indices in the subset\n",
        "remaining_indices = subset_df.index.difference(indices_to_set_true)\n",
        "df.loc[remaining_indices, 'use'] = False\n",
        "\n",
        "# Set 'use' to False where shape is not (256, 256, 3)\n",
        "df.loc[df['shape'] != (256, 256, 3), 'use'] = False\n",
        "\n",
        "# Optional: Verify the changes\n",
        "print(\"Count of 'True' in 'use' where polygon_count is 0:\", df[(df['polygon_count'] == 0) & (df['use'])].shape[0])\n",
        "print(\"Count of 'False' in 'use' where polygon_count is 0:\", df[(df['polygon_count'] == 0) & (~df['use'])].shape[0])\n",
        "print('Percentage of rows where polygon count is 0:', round(100*df[df['polygon_count'] == 0].shape[0]/df.shape[0], 1))\n",
        "print('Percentage of rows where poloygon count is 0 and use is True:', round(100*df[(df['polygon_count'] == 0) & (df['use'])].shape[0]/df.shape[0], 1))\n",
        "print('\\nTotal number of rows:', df.shape[0])\n",
        "\n",
        "# Check for duplicate filenames\n",
        "duplicates = df.duplicated(subset='fwp')\n",
        "duplicated_files = df[duplicates]\n",
        "print(f'\\nNumber of duplicated files: {duplicated_files.shape[0]}')\n",
        "\n",
        "# Print the head of the df\n",
        "print(f'\\n{df.head()}')"
      ],
      "metadata": {
        "id": "02WQ3Qj58Giq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  WE DO NOT NEED THIS ONE\n",
        "\n",
        "def aggregate_data(df):\n",
        "    \"\"\"\n",
        "    Aggregates the data in the given DataFrame by grouping it based on the 'base_folder' column.\n",
        "    Calculates the count of images, sum of 'use' column, and count of 'polygon_count' where the value is 0.\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame): The DataFrame containing the data to be aggregated.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: The aggregated DataFrame with columns 'img_count', 'use', and 'z_count'.\n",
        "    \"\"\"\n",
        "    # Group by 'base_folder' and aggregate\n",
        "    df_bf = df.groupby('base_folder').agg(\n",
        "        img_count=('cwp', 'count'),\n",
        "        use=('use', 'sum'),\n",
        "        z_count=('polygon_count', lambda x: (x == 0).sum())\n",
        "    )\n",
        "\n",
        "    return df_bf\n",
        "\n",
        "# Aggregate data for analysis\n",
        "aggregated_df = aggregate_data(df)\n",
        "print(aggregated_df[['img_count', 'use', 'z_count']])\n",
        "\n",
        "# Print the total number of files\n",
        "print(f'\\n\\tTotal: \\t\\t{df.shape[0]}')\n",
        "\n",
        "# Print the number of files to use\n",
        "print(f'\\tUsed: \\t\\t{df[df[\"use\"]].shape[0]}')\n",
        "print(f'\\tNot Used: \\t{df.shape[0] - df[df[\"use\"]].shape[0]}')"
      ],
      "metadata": {
        "id": "SfambxMX8gA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actual preprocessing"
      ],
      "metadata": {
        "id": "pVoVG8c9LFjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add part to loop into all the cities list\n",
        "# Add part to obtain dataset split from the existing files structure\n",
        "\n",
        "# Call the find_files function and print the number of found files\n",
        "cwps, lwps, base_folders = find_files()\n",
        "print('Found {} chip files'.format(len(cwps)))\n",
        "N = 2\n",
        "test_list_cwps = cwps[:N]\n",
        "print(f\"The first {N} elements of list are : {str(test_list_cwps)}\")\n",
        "print('Found {} label files\\n'.format(len(lwps)))\n",
        "test_list_lwps = lwps[:N]\n",
        "print(f\"The first {N} elements of list are : {str(test_list_lwps)}\")\n",
        "# Print message if all filenames match\n",
        "print('All filenames match; each tif has a label!')\n",
        "\n",
        "# Call the shapes_data and print the shape of the first chip file\n",
        "shapes_data = check_shapes(cwps)\n",
        "print(f'Chip shapes with counts are: {shapes_data[0]}')"
      ],
      "metadata": {
        "id": "-3uXzikDcGbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data it into train-val-test and convert label and image formats for YOLO model training.\n",
        "# 1. Define SEED and train-val-test split percentages\n",
        "# 2. Get the chip filenames where 'use'=True\n",
        "# 3. Shuffle and divide the chips to prevent biased data splitting.\n",
        "# 4. Confirm the training, validation, and testing arrays to verify the correct split.\n",
        "# 8. Write YOLO label files.\n",
        "# 9. Convert image files from TIFF to JPEG.\n",
        "\n",
        "# SEED = 42\n",
        "# TRAIN_SPLIT = 0.7 ... not needed!\n",
        "# VAL_SPLIT = 0.15\n",
        "# TEST_SPLIT = 0.15\n",
        "\n",
        "# assert(TRAIN_SPLIT + VAL_SPLIT + TEST_SPLIT == 1), \"The sum of the splits must be equal to 1\"\n",
        "\n",
        "# print(f'Train-val-test split: {TRAIN_SPLIT}-{VAL_SPLIT}-{TEST_SPLIT}')\n",
        "\n",
        "# # Set the random seed\n",
        "# np.random.seed(SEED)\n",
        "\n",
        "# # Get the cwps\n",
        "# cwps = df[df['use']]['cwp'].values\n",
        "\n",
        "# # Shuffle indices\n",
        "# shuffled_indices = np.random.permutation(len(cwps))\n",
        "\n",
        "# # Calculate split indices for the specified split percentage\n",
        "# train_end = int(len(cwps) * TRAIN_SPLIT)\n",
        "# val_end = train_end + int(len(cwps) * VAL_SPLIT)\n",
        "\n",
        "# # Split the indices into training, validation, and testing\n",
        "# train_indices = shuffled_indices[:train_end]\n",
        "# val_indices = shuffled_indices[train_end:val_end]\n",
        "# test_indices = shuffled_indices[val_end:]\n",
        "\n",
        "# Create train, val, and test arrays\n",
        "train_cwps = cwps[train_indices]\n",
        "val_cwps = cwps[val_indices]\n",
        "test_cwps = cwps[test_indices]\n",
        "\n",
        "# Output the results to verify\n",
        "print(f'\\nTrain array size: {len(train_cwps)}')\n",
        "print(f'Validation array size: {len(val_cwps)}')\n",
        "print(f'Test array size: {len(test_cwps)}\\n')\n",
        "\n",
        "# Check if the YOLO folder exists, if not create labels, images, and folders\n",
        "if not os.path.exists('ramp_data_yolo'):\n",
        "    # Create the folder\n",
        "    os.makedirs('ramp_data_yolo')\n",
        "\n",
        "    # Write the YOLO label files for the training set\n",
        "    print('Generating training labels')\n",
        "    for train_cwp in tqdm(train_cwps):\n",
        "        write_yolo_file(train_cwp, 'train')\n",
        "\n",
        "    # Write the YOLO label files for the validation set\n",
        "    print('Generating validation labels')\n",
        "    for val_cwp in tqdm(val_cwps):\n",
        "        write_yolo_file(val_cwp, 'val')\n",
        "\n",
        "    # Write the YOLO label files for the test set\n",
        "    print('Generating test labels')\n",
        "    for test_cwp in tqdm(test_cwps):\n",
        "        write_yolo_file(test_cwp, 'test')\n",
        "\n",
        "    # Convert the chip files to JPEG format\n",
        "    print('Generating training images')\n",
        "    for train_cwp in tqdm(train_cwps):\n",
        "        convert_tif_to_jpg(train_cwp, 'train')\n",
        "\n",
        "    print('Generating validation images')\n",
        "    for val_cwp in tqdm(val_cwps):\n",
        "        convert_tif_to_jpg(val_cwp, 'val')\n",
        "\n",
        "    print('Generating test images')\n",
        "    for test_cwp in tqdm(test_cwps):\n",
        "        convert_tif_to_jpg(test_cwp, 'test')\n",
        "\n",
        "else:\n",
        "    print('Data already converted')"
      ],
      "metadata": {
        "id": "oQxgxVXyLI8_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}